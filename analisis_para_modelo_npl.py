# -*- coding: utf-8 -*-
"""Analisis_para_modelo_npl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12e6x87q2n5qe06OdRoxXKKNTJPNsNmOV
"""

pip install num2words

import unicodedata
import random
import gensim
import pandas as pd
import numpy as np
import multiprocessing as mp

import matplotlib.pyplot as plt
from wordcloud import WordCloud

from collections import Counter

import nltk
from nltk import ngrams
from nltk import RegexpTokenizer
from nltk.probability import FreqDist
from nltk.stem import WordNetLemmatizer

from num2words import num2words

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import _stop_words
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import chi2
from sklearn.ensemble import GradientBoostingClassifier

from tensorflow.keras.layers import (
    Dense,
    Dropout,
    Embedding,
    LSTM,
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.optimizers import Adam

from keras.preprocessing import sequence

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

name_file = '/content/result_anfx_clients_pro.csv'
df_client = pd.read_csv(name_file)

print(df_client.columns)
print(len(df_client.columns))
df_client.head()

"""Mostraremos la distribución del nps"""

df_nps = df_client['nps-score']
df_nps = df_nps.dropna()

count_to_nps = df_nps.value_counts()

nps = df_nps.unique()

plt.barh(nps, count_to_nps)
plt.title('Distribución de reviews por estrellas')
plt.xlabel('Nº de nps')
plt.ylabel('NPS')
plt.show()

"""Distribución por el semáforo"""

semaphore_green = len(df_client[df_client['nps-score'] >= 9])
semaphore_yellow = len(df_client[(df_client['nps-score'] >= 6) & (df_client['nps-score'] <= 8)])
semaphore_red = len(df_client[df_client['nps-score'] <= 5])

plt.barh(['Green','Yellow', 'Red'],[semaphore_green, semaphore_yellow, semaphore_red])
plt.title('Distribución clientes semaforo')
plt.xlabel('Nº de nps')
plt.show()

"""Analisis de texto según lo tenemos en el data set"""

# Metodo para obtener el numero total de reviews
def total_number_of_reviews(df):
  df_aux = df.dropna()
  return df_aux.shape[0]

number_feedback = total_number_of_reviews(df_client['nps-feedback'])
print(f'Tenemos un total de {number_feedback} feedback.')

# Metodo para obtener el numero total de frases
def total_number_of_sentences(df):
  sentences = list()
  for review in df.dropna().values:
    for sentnce in review[0].split('.'):
      sentences.append(sentnce)

  return len(sentences)

number_sentences = total_number_of_sentences(df_client['nps-feedback'])
print(f'Tenemos un total de {number_sentences} frases.')

# Metodo para obtener el numero total de tokens o palabras
def total_number_of_tokens(df):
  tokens = list()
  for review in df.dropna().values:
    for token in review[0].split():
      tokens.append(token)

  return len(tokens)

number_tokens = total_number_of_tokens(df_client[['nps-feedback']])
print(f'Tenemos un total de {number_tokens} palabras o tokens.')

# Metodo para obtener los tokens o palabras
def extract_tokens_from_a_DataFrame(df):
  tokens = list()
  for review in df.dropna().values:
    for token in review[0].split():
      tokens.append(token)

  return tokens

# Almacenaremos el total de los tokens
words_totals = extract_tokens_from_a_DataFrame(df_client[['nps-feedback']])
words_totals = [x.lower() for x in words_totals]

# Metodo para generar un grafico de tipo nuve de palabras
def plot_word_cloud(text):
    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(' '.join(text))
    plt.figure(figsize=(12,6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

count_words = Counter(words_totals)
w_most_common = count_words.most_common(10)
words = [w[0] for w in w_most_common]
freqs = [w[1] for w in w_most_common]
freqs, words = zip(*sorted(zip(freqs, words)))
plt.barh(words, freqs)
plt.title('Palabras más comunes')
plt.xlabel('Veces repetidas')
plt.ylabel('Palabras')
plt.show()

# generamos los n-grams
bigrams_ = list(ngrams(words_totals, 2))
trigrams_ = list(ngrams(words_totals, 3))

# calculamos la frecuencias
bg_freq = FreqDist(bigrams_)
tg_freq = FreqDist(trigrams_)

bg_freq_most_common = bg_freq.most_common(10)
bgs_ = [str(bg[0]) for bg in bg_freq_most_common]
bgs_f_ = [bg[1] for bg in bg_freq_most_common]

tg_freq_most_common = tg_freq.most_common(10)
tgs_ = [str(tg[0]) for tg in tg_freq_most_common]
tgs_f_ = [tg[1] for tg in tg_freq_most_common]

bgs_f_, bgs_ = zip(*sorted(zip(bgs_f_, bgs_)))
tgs_f_, tgs_ = zip(*sorted(zip(tgs_f_, tgs_)))

plt.barh(bgs_, bgs_f_)
plt.title('Bigram frequencies')
plt.show()

plt.barh(tgs_, tgs_f_)
plt.title('Trigram frequencies')
plt.show()

plot_word_cloud(words_totals)

"""Sin stop words"""

# Lista con las Stop Words
stopword_es = nltk.corpus.stopwords.words('spanish')

# Almacenaremos el total de los tokens sin Stop Words
words_totals_no_sw = [word for word in words_totals if word not in stopword_es]

"""El número de reviews y de frases será el mismo que antes pero el número de palabras es el que va a cambiar, así como los N-grams o la nuve de palabras, al eliminar las Stop Words de nuestras reviews."""

print(f'Tenemos un total de {len(words_totals_no_sw)} palabras o tokens sin las Stop Words.')

count_words_no_ws = Counter(words_totals_no_sw)
w_most_common = count_words_no_ws.most_common(10)
words = [w[0] for w in w_most_common]
freqs = [w[1] for w in w_most_common]
freqs, words = zip(*sorted(zip(freqs, words)))
plt.barh(words, freqs)
plt.title('Palabras más comunes')
plt.xlabel('Veces repetidas')
plt.ylabel('Palabras')
plt.show()

# N-grams

# generamos los n-grams
bigrams_no_sw = list(ngrams(words_totals_no_sw, 2))
trigrams_no_sw = list(ngrams(words_totals_no_sw, 3))

# calculamos la frecuencias
bg_freq_no_sw = FreqDist(bigrams_no_sw)
tg_freq_no_sw = FreqDist(trigrams_no_sw)

bg_freq_most_common_no_sw = bg_freq_no_sw.most_common(10)
bgs_no_sw = [str(bg[0]) for bg in bg_freq_most_common_no_sw]
bgs_f_no_sw = [bg[1] for bg in bg_freq_most_common_no_sw]

tg_freq_most_common_no_sw = tg_freq_no_sw.most_common(10)
tgs_no_sw = [str(tg[0]) for tg in tg_freq_most_common_no_sw]
tgs_f_no_sw = [tg[1] for tg in tg_freq_most_common_no_sw]

bgs_f_no_sw, bgs_no_sw = zip(*sorted(zip(bgs_f_no_sw, bgs_no_sw)))
tgs_f_no_sw, tgs_no_sw = zip(*sorted(zip(tgs_f_no_sw, tgs_no_sw)))

plt.barh(bgs_no_sw, bgs_f_no_sw)
plt.title('Bigram frequencies sin stop words')
plt.show()

plt.barh(tgs_no_sw, tgs_f_no_sw)
plt.title('Trigram frequencies sin stop words')
plt.show()

plot_word_cloud(words_totals_no_sw)

"""Etapa del procesado de texto"""

# Método eliminación de acentos, etc
def regularize_unicode(text):
  clean_text = ''
  clean_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  return clean_text

# Método para eliminar stop words
def delete_stop_words(text, sw_list=nltk.corpus.stopwords.words('spanish')):
  clean_text = ''
  clean_text = ' '.join([word for word in text.lower().split() if word not in sw_list])
  return clean_text


# Método para regularizar en mayusculas o minusculas
def regularize_lowercase(text, tokenizer):
  clean_text = list()

  for word in tokenizer.tokenize(text):
    clean_word = word.lower()
    clean_text.append(clean_word)

  return ' '.join(clean_text)


# Método lematizar
def regularize_lemat(text, tokenizer, lemmatizer):
  clean_text = list()

  for word in tokenizer.tokenize(text):
    clean_word = lemmatizer.lemmatize(word)
    clean_text.append(clean_word)

  return ' '.join(clean_text)


# Método para eliminar espacios sobrantes
import string
def regularize_spaces(text):
  table = str.maketrans('', '', string.punctuation)
  clean_text = ' '.join([word.translate(table) for word in text.split()])
  return clean_text


# Método convertir dígitos a palabras
def regularize_convert_digits_to_words(text, tokenizer):
  clean_text = list()

  for word in tokenizer.tokenize(text):
    if word.isdigit():
      clean_word = num2words(word, lang='es')
      clean_text.append(clean_word)
    else:
      clean_text.append(word)

  return ' '.join(clean_text)

# Método de procesamiento general del texto
def pipeline_cleaner(text_list):

  clean_text = list()
  tokenizer = RegexpTokenizer(r'\w+')
  lemmatizer = WordNetLemmatizer()

  for i in range(len(text_list)):
    text_clean = regularize_unicode(text_list[i])
    text_clean = regularize_lowercase(text_clean, tokenizer)
    text_clean = delete_stop_words(text_clean, stopword_es)
    text_clean = regularize_lemat(text_clean, tokenizer, lemmatizer)
    text_clean = regularize_convert_digits_to_words(text_clean, tokenizer)
    text_clean = regularize_spaces(text_clean)
    clean_text.append(text_clean)

  return clean_text

df_client_clean = df_client[df_client['nps-feedback'].notna()]

nps_feedback = pipeline_cleaner(df_client_clean['nps-feedback'].tolist())
nps_score = df_client_clean['nps-score']

df_model = pd.DataFrame({
    'feedback': nps_feedback,
    'score': nps_score
})
df_model.dropna(subset=['feedback', 'score'], inplace=True)
df_model.reset_index(drop=True, inplace=True)

stopword_es.append('1o')
stopword_es.append('2o')

df_model.head(10)

# Metodo por el que a partir de la puntuación lo convertimos en semaforo
def label_semaphore(row):
  if int(row['score']) <= 6:
    return 1
  else:
    return 0

df_model["semaphore"] = df_model.apply(lambda row: label_semaphore(row), axis=1)

df_model.head(10)

# Almacenaremos el total de los tokens
words_totals_processed = extract_tokens_from_a_DataFrame(df_model[['feedback']])

count_words_processed = Counter(words_totals_processed)
w_most_common = count_words_processed.most_common(10)
words = [w[0] for w in w_most_common]
freqs = [w[1] for w in w_most_common]
freqs, words = zip(*sorted(zip(freqs, words)))
plt.barh(words, freqs)
plt.title('Palabras más comunes')
plt.xlabel('Veces repetidas')
plt.ylabel('Palabras')
plt.show()

# N-grams

# generamos los n-grams
bigrams_no_sw = list(ngrams(count_words_processed, 2))
trigrams_no_sw = list(ngrams(count_words_processed, 3))

# calculamos la frecuencias
bg_freq_no_sw = FreqDist(bigrams_no_sw)
tg_freq_no_sw = FreqDist(trigrams_no_sw)

bg_freq_most_common_no_sw = bg_freq_no_sw.most_common(15)
bgs_no_sw = [str(bg[0]) for bg in bg_freq_most_common_no_sw]
bgs_f_no_sw = [bg[1] for bg in bg_freq_most_common_no_sw]

tg_freq_most_common_no_sw = tg_freq_no_sw.most_common(15)
tgs_no_sw = [str(tg[0]) for tg in tg_freq_most_common_no_sw]
tgs_f_no_sw = [tg[1] for tg in tg_freq_most_common_no_sw]

bgs_f_no_sw, bgs_no_sw = zip(*sorted(zip(bgs_f_no_sw, bgs_no_sw)))
tgs_f_no_sw, tgs_no_sw = zip(*sorted(zip(tgs_f_no_sw, tgs_no_sw)))

plt.barh(bgs_no_sw, bgs_f_no_sw)
plt.title('Bigram frequencies sin stop words')
plt.show()

plt.barh(tgs_no_sw, tgs_f_no_sw)
plt.title('Trigram frequencies sin stop words')
plt.show()

plot_word_cloud(count_words_processed)

"""##3. Etapa de entrenamiento y testeo de un modelo de análisis de sentimiento

Vamos a realizar el split de los datos con los que vamos a trabajar. El conjunto de datos no es muy grande por lo que solo vamos a dividir en Train y Test
"""

X_train, X_test, y_train, y_test = train_test_split(
    df_model['feedback'],
    df_model['semaphore'],
    train_size=0.90,
    test_size=0.10,
    random_state=42,
    shuffle=True
)

# Comprobamos que en la parte de data frame de entrenamiento tenemos solo los textos
X_train.iloc[:10]

# Comprobamos que en la parte de data frame de etiquetas tenemos solo la parte binaria
y_train.iloc[:10]

"""Como vamos a trabajar con el sentimiento de los textos, en un clasificador, es buena practica utilizar n-gramas puesto que nos ayuda a contextualizar los textos y esto mejora las capacidades del modelo a la hora de predecir el sentimiento."""

cv = CountVectorizer(
    max_df=0.95,
    min_df=1,
    max_features=1000, #1000
    strip_accents='ascii',
    ngram_range=(1, 3)
)
cv.fit(X_train)

# aplicamos el modelo entrenado a los datos para obtener su representacion 
X_train_ = cv.transform(X_train)
X_test_ = cv.transform(X_test)

"""#Análisis de sentimiento con ML (LogisticRegression)

Vamos a entrenar el modelo con varias configuraciones para ver cual tiene mejor rendimiento.
"""

c_params = [0.01, 0.05, 0.25, 0.5, 1, 10, 100, 1000, 10000]

test_pred = list()
train_acc = list()
test_acc = list()

for c in c_params:
    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1500)
    lr.fit(X_train_, y_train)
    
    train_predict = lr.predict(X_train_)
    test_predict = lr.predict(X_test_)
    
    print ("Accuracy for C={}: {}".format(c, accuracy_score(y_test, test_predict)))
    
    train_acc.append(accuracy_score(y_train, train_predict))
    test_acc.append(accuracy_score(y_test, test_predict))

    test_pred.append(test_predict)

result_test_predict = test_pred[7]

print('Confussion matrix:\n{}'.format(confusion_matrix(y_test, result_test_predict)))
print('\nClassification report:\n{}'.format(classification_report(y_test, result_test_predict)))
print('Accuracy score:{}'.format(accuracy_score(y_test, result_test_predict)))

plt.figure(figsize=(12, 8))
plt.plot(train_acc, label='train')
plt.plot(test_acc, label='test')
plt.axvline(np.argmax(test_acc), c='g', ls='--', alpha=0.8)
plt.title('Accuracy evolution for different C values')
plt.xlabel('C')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.xticks(list(range(len(c_params))), c_params)
plt.tight_layout()
plt.show()

"""##Modelo seleccionado para probarlo."""

modelo_ok = LogisticRegression(C=1000, solver='lbfgs', max_iter=1500)
modelo_ok.fit(X_train_, y_train)

def predict_review_sentiment(review_index, model):
    print('Actual sentiment: {}'.format(df_model.iloc[review_index]['semaphore']))
    r = df_model.iloc[review_index]['feedback']
    print('Prediction: {}'.format(modelo_ok.predict(cv.transform([r]))))

for i in random.sample(range(0, len(df_model)), 10):
    print('\nReview no. {}'.format(i))
    predict_review_sentiment(i, modelo_ok)